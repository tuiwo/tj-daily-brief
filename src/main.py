import os
import re
import time
import math
import smtplib
import datetime as dt
from zoneinfo import ZoneInfo
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import json

import requests
import yaml


# -------------------------
# åŸºç¡€ï¼šè¯»å– config
# -------------------------
def load_config(path="config.yml"):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def now_local(tz: str) -> dt.datetime:
    return dt.datetime.now(ZoneInfo(tz))


# def should_send_now(cfg) -> bool:
#     # åªåœ¨æœ¬åœ°æŒ‡å®šå°æ—¶å‘ä¿¡ï¼ˆç”¨äºé…åˆ UTC 15/16 åŒè·‘ï¼‰
#     return now_local(cfg["timezone"]).hour == int(cfg["send_hour_local"])
def should_send_now(cfg) -> bool:
    now = now_local(cfg["timezone"])
    print(f"DEBUG tz={cfg['timezone']} now={now.isoformat()} hour={now.hour} send_hour_local={cfg['send_hour_local']}")
    return now.hour == int(cfg["send_hour_local"])

# -------------------------
# OpenAlexï¼šæŠ½è±¡è¿˜åŸï¼ˆabstract_inverted_index -> stringï¼‰
# -------------------------
def reconstruct_abstract(inv_idx):
    if not inv_idx:
        return ""
    pos2word = {}
    for word, poses in inv_idx.items():
        for p in poses:
            pos2word[p] = word
    return " ".join(pos2word[i] for i in sorted(pos2word))


def openalex_get(params):
    r = requests.get("https://api.openalex.org/works", params=params, timeout=60)
    r.raise_for_status()
    return r.json()


def openalex_get_work_by_id(openalex_id: str, mailto: str = "") -> dict | None:
    """
    openalex_id é€šå¸¸é•¿è¿™æ ·ï¼š
      https://openalex.org/Wxxxxxxxxx
    æˆ‘ä»¬æŠŠå®ƒè½¬æ¢ä¸º APIï¼š
      https://api.openalex.org/works/Wxxxxxxxxx
    """
    if not openalex_id:
        return None
    oid = openalex_id.strip()
    if oid.startswith("https://openalex.org/"):
        work_id = oid.split("/")[-1]  # Wxxxx
    else:
        work_id = oid  # ä¹Ÿå¯èƒ½ç›´æ¥ç»™ Wxxxx

    url = f"https://api.openalex.org/works/{work_id}"
    params = {}
    if mailto:
        params["mailto"] = mailto
    r = requests.get(url, params=params, timeout=60)
    if r.status_code == 404:
        return None
    r.raise_for_status()
    return r.json()


def normalize_doi(doi: str) -> str:
    """
    OpenAlex çš„ doi å­—æ®µä¸€èˆ¬æ˜¯å®Œæ•´ URL å½¢å¼ï¼šhttps://doi.org/...
    è¿™é‡ŒæŠŠç”¨æˆ·è¾“å…¥çš„ DOI è§„èŒƒæˆè¿™ç§å½¢å¼ï¼Œä¾¿äº filter=doi:...
    """
    d = (doi or "").strip()
    if not d:
        return ""
    d = d.lower()
    d = d.replace("doi:", "").strip()
    if d.startswith("http://"):
        d = "https://" + d[len("http://"):]
    if d.startswith("https://doi.org/"):
        return d
    return "https://doi.org/" + d


def openalex_find_work_by_doi(doi: str, mailto: str = "") -> dict | None:
    """
    ç”¨ filter=doi:... æ‰¾åˆ°å¯¹åº” work
    """
    doi_url = normalize_doi(doi)
    if not doi_url:
        return None
    params = {"filter": f"doi:{doi_url}", "per_page": 1}
    if mailto:
        params["mailto"] = mailto
    data = openalex_get(params)
    results = data.get("results", [])
    return results[0] if results else None


def pick_best_url(work: dict) -> str:
    doi = work.get("doi")
    if doi:
        return doi
    primary = (work.get("primary_location") or {}).get("landing_page_url")
    if primary:
        return primary
    return work.get("id", "")


def normalize(s: str) -> str:
    return (s or "").lower()


# -------------------------
# ç›¸å…³æ€§ä¸è§„åˆ™æ‘˜è¦
# -------------------------
def relevance_score(title: str, abstract: str, keywords: list[str]) -> int:
    t = normalize(title)
    a = normalize(abstract)
    score = 0
    for kw in keywords:
        k = kw.lower()
        if k in t:
            score += 3
        elif k in a:
            score += 1
    return score


def excluded(title: str, abstract: str, exclude_keywords: list[str]) -> bool:
    t = normalize(title)
    a = normalize(abstract)
    return any(k.lower() in t or k.lower() in a for k in exclude_keywords)


def extract_numbers(text: str) -> str:
    hits = re.findall(r"(\d+(?:\.\d+)?\s*(?:Â°c|â„ƒ|k|%))", normalize(text))
    uniq = []
    for h in hits:
        h = h.replace(" ", "")
        if h not in uniq:
            uniq.append(h)
    return ", ".join(uniq[:6])


def guess_tags(text: str) -> list[str]:
    t = normalize(text)
    tags = []
    mapping = [
        ("TSEP", ["tsep", "temperature sensitive electrical parameter"]),
        ("ç”µå‚æ³•(Vce/Vf/Rds)", ["vce", "vce(sat)", "vf", "forward voltage", "rds(on)"]),
        ("ç”µçƒ­æ¨¡å‹/çƒ­é˜»æŠ—", ["electro-thermal", "thermal impedance", "foster", "cauer"]),
        ("æ»¤æ³¢/ä¼°è®¡", ["kalman", "ukf", "ekf", "observer", "state estimation"]),
        ("å™¨ä»¶:SiC", ["sic"]),
        ("å™¨ä»¶:IGBT", ["igbt"]),
        ("æ¨¡å—/å°è£…", ["power module", "module", "packaging"]),
    ]
    for name, keys in mapping:
        if any(k in t for k in keys):
            tags.append(name)
    return tags[:4]


def human_brief_cn(title: str, abstract: str) -> str:
    tags = guess_tags(title + " " + abstract)
    nums = extract_numbers(abstract)

    sents = re.split(r"(?<=[.!?])\s+", abstract.strip())
    sents = [s for s in sents if len(s) > 40]
    explain = " ".join(sents[:2]) if sents else "ï¼ˆæ‘˜è¦ä¿¡æ¯ä¸è¶³ï¼šå»ºè®®ç‚¹å¼€é“¾æ¥å¿«é€Ÿåˆ¤æ–­æ˜¯å¦ä¸ä½ çš„åœ¨çº¿ç›‘æµ‹é“¾è·¯ç›¸å…³ã€‚ï¼‰"

    return "\n".join([
        "ä¸€å¥è¯ï¼šè¿™ç¯‡å·¥ä½œå›´ç»•ç»“æ¸©åœ¨çº¿ä¼°ç®—/ç›‘æµ‹ç»™å‡ºä¸€æ¡å¯å®ç°çš„æŠ€æœ¯è·¯å¾„ã€‚",
        f"æ–¹æ³•çº¿ç´¢ï¼š{(' / '.join(tags)) if tags else 'æœªä»æ‘˜è¦é‡Œè¯†åˆ«åˆ°æ˜ç¡®æ–¹æ³•å…³é”®è¯'}",
        f"å¯é‡åŒ–æŒ‡æ ‡ï¼š{nums if nums else 'æ‘˜è¦æœªç»™å‡ºæ˜ç¡®æ•°å€¼ï¼ˆæˆ–éœ€è¯»å…¨æ–‡/å›¾è¡¨ï¼‰'}",
        f"æ‹†è§£ï¼š{explain}",
        "å»ºè®®ï¼šå¦‚æœä½ åœ¨åš TSEP æ ‡å®š/åœ¨çº¿ä¼°ç®—é“¾è·¯/è¯¯å·®è¯„ä¼°ï¼Œè¿™ç¯‡ä¼˜å…ˆè¯»ï¼›å¦åˆ™å…ˆæ”¶è—è§‚å¯Ÿã€‚"
    ])


# -------------------------
# å€™é€‰è·å–ï¼šå…³é”®è¯ï¼ˆæœ€æ–°/ç»å…¸ï¼‰
# -------------------------
def fetch_latest_and_classic(cfg, mailto: str):
    query = cfg.get("search_query") or " ".join(cfg["keywords"][:6])

    today = dt.date.today()
    from_date = (today - dt.timedelta(days=int(cfg["latest_days"]))).isoformat()
    classic_to = (today - dt.timedelta(days=365 * 2)).isoformat()

    common_filter = "type:journal-article|proceedings-article"

    base = {"search": query, "per_page": 50}
    if mailto:
        base["mailto"] = mailto

    latest = openalex_get({
        **base,
        "filter": f"from_publication_date:{from_date},{common_filter}",
        "sort": "publication_date:desc",
    }).get("results", [])

    classic = openalex_get({
        **base,
        "filter": f"to_publication_date:{classic_to},{common_filter}",
        "sort": "cited_by_count:desc",
    }).get("results", [])

    return latest, classic


# -------------------------
# Milestone Bï¼šDOI seeds -> related_works æ¨è
# -------------------------
def load_seed_dois(path: str) -> list[str]:
    if not os.path.exists(path):
        return []
    out = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            s = line.strip()
            if not s or s.startswith("#"):
                continue
            out.append(s)
    return out


def fetch_recommendations_from_seeds(cfg, mailto: str) -> list[dict]:
    """
    å¯¹æ¯ä¸ª seed DOIï¼š
      DOI -> OpenAlex work
      work.related_works -> æ‹‰å›ç›¸å…³ works
    æœ€ååˆå¹¶å»é‡ï¼Œå¹¶æ‰“ä¸Š reco_source æ ‡è®°
    """
    pos = load_seed_dois("seeds_positive.txt")
    neg = set(normalize_doi(x) for x in load_seed_dois("seeds_negative.txt"))

    if not pos:
        return []

    max_related = int(cfg.get("max_related_per_seed", 25))
    all_ids: list[str] = []
    seed_doi_urls = set()

    # 1) æ¯ä¸ª DOI æ‰¾åˆ°å¯¹åº” workï¼Œå¹¶æ”¶é›† related_works ids
    for doi in pos:
        w = openalex_find_work_by_doi(doi, mailto)
        time.sleep(0.12)  # è½»å¾®é™é€Ÿï¼Œå‡å°‘è¢«é™æµé£é™©
        if not w:
            continue
        doi_url = w.get("doi")
        if doi_url:
            seed_doi_urls.add(doi_url)

        rel = w.get("related_works") or []
        all_ids.extend(rel[:max_related])

    # 2) æ‹‰å› related works è¯¦æƒ…ï¼ˆé€ä¸ªæ‹‰ï¼Œé‡ä¸å¤§æ›´ç¨³ï¼‰
    recos = []
    seen = set()
    for oid in all_ids:
        if oid in seen:
            continue
        seen.add(oid)
        w = openalex_get_work_by_id(oid, mailto)
        time.sleep(0.12)
        if not w:
            continue
        # æ’é™¤ï¼šè´Ÿä¾‹ DOIã€ä»¥åŠç§å­æœ¬èº«
        doi_url = w.get("doi") or ""
        if doi_url and (doi_url in neg or doi_url in seed_doi_urls):
            continue
        recos.append(w)

    return recos



# -------------------------
# milestone C: wo API
# -------------------------

def s2_headers():
    # æ²¡ key ä¹Ÿèƒ½å°è¯•ï¼›æœ‰ key ä¼šæ›´ç¨³ï¼ˆå®˜æ–¹å»ºè®®ä½¿ç”¨ keyï¼‰ [oai_citation:4â€¡Semantic Scholar](https://www.semanticscholar.org/product/api%2Ftutorial?utm_source=chatgpt.com)
    key = (os.getenv("S2_API_KEY") or "").strip()
    h = {"Content-Type": "application/json"}
    if key:
        h["x-api-key"] = key
    return h


def doi_to_s2_pid(doi: str) -> str:
    """
    æŠŠ DOI è§„èŒƒæˆ Semantic Scholar æ¨èæ¥å£å¸¸ç”¨çš„ paperId å½¢å¼ï¼šDOI:10.xxxx/xxxx
    """
    d = (doi or "").strip()
    d = d.replace("doi:", "").strip()
    d = d.replace("https://doi.org/", "").strip()
    d = d.replace("http://doi.org/", "").strip()
    return f"DOI:{d}" if d else ""


def fetch_s2_recommendations_from_seeds(cfg) -> list[dict]:
    """
    Semantic Scholar Recommendations APIï¼š
      POST https://api.semanticscholar.org/recommendations/v1/papers
    å®˜æ–¹æœ‰ Recommendations API æ–‡æ¡£ã€‚ [oai_citation:5â€¡è¯­ä¹‰å­¦è€…](https://api.semanticscholar.org/api-docs/recommendations?utm_source=chatgpt.com)

    æ—  keyï¼šæ›´å¯èƒ½ 429/å¤±è´¥ï¼Œæ‰€ä»¥è¿™é‡Œåšï¼š
      - å° limit
      - é‡è¯• + æŒ‡æ•°é€€é¿
      - å¤±è´¥ç›´æ¥è¿”å›ç©ºåˆ—è¡¨ï¼ˆä¸å½±å“é‚®ä»¶ï¼‰
    """
    if not cfg.get("use_s2_recommendations", True):
        return []

    # å…ˆå°è¯• ai4scholarï¼šæˆåŠŸå°±ç›´æ¥ç”¨å®ƒï¼Œè·³è¿‡å®˜æ–¹ S2
    ok, recs = fetch_ai4s_recommendations_from_seeds(cfg)
    if ok:
        print(f"AI4S: used, recs={len(recs)} (skip official S2)")
        return recs



    
    
    pos = load_seed_dois("seeds_positive.txt")
    neg = load_seed_dois("seeds_negative.txt")

    positive = [doi_to_s2_pid(d) for d in pos if doi_to_s2_pid(d)]
    negative = [doi_to_s2_pid(d) for d in neg if doi_to_s2_pid(d)]

    if not positive:
        return []

    url = "https://api.semanticscholar.org/recommendations/v1/papers/"
    params = {
        "fields": "title,abstract,year,citationCount,venue,externalIds,url",
        "limit": int(cfg.get("s2_limit", 20)),
    }

    payload = {"positivePaperIds": positive, "negativePaperIds": negative}

    retries = int(cfg.get("s2_retries", 2))
    base_backoff = int(cfg.get("s2_backoff_sec", 3))

    for attempt in range(retries + 1):
        try:
            print(f"S2: start, positive={len(positive)}, negative={len(negative)}, limit={params['limit']}, has_key={bool((os.getenv('S2_API_KEY') or '').strip())}")
            r = requests.post(
                url,
                headers=s2_headers(),
                params=params,
                data=json.dumps(payload),
                timeout=60,
            )

            # 429/5xxï¼šé‡è¯•ï¼ˆæ—  key æ—¶æ›´å¸¸è§ï¼‰ [oai_citation:6â€¡Semantic Scholar](https://www.semanticscholar.org/product/api%2Ftutorial?utm_source=chatgpt.com)
            if r.status_code == 429 or 500 <= r.status_code < 600:
                if attempt < retries:
                    sleep_s = base_backoff * (2 ** attempt)
                    print(f"S2 rate-limited or server error ({r.status_code}); retry in {sleep_s}s")
                    time.sleep(sleep_s)
                    continue
                print(f"S2 failed with status={r.status_code}; skipping.")
                return []

            r.raise_for_status()
            data = r.json()
            recs = data.get("recommendedPapers", []) or []
            print(f"S2: ok, status={r.status_code}, recs={len(recs)}")
            return recs
            return data.get("recommendedPapers", []) or []

        except Exception as e:
            if attempt < retries:
                sleep_s = base_backoff * (2 ** attempt)
                print(f"S2 exception: {e}; retry in {sleep_s}s")
                time.sleep(sleep_s)
                continue
            print(f"S2 exception: {e}; skipping.")
            return []

    return []





def bare_doi(doi_or_url: str) -> str:
    """
    è¾“å…¥å¯èƒ½æ˜¯ï¼š
      - https://doi.org/10.xxx/yyy  ï¼ˆOpenAlex å¸¸è§ï¼‰
      - DOI:10.xxx/yyy
      - 10.xxx/yyy
    è¾“å‡ºç»Ÿä¸€ä¸ºï¼š10.xxx/yyy
    """
    s = (doi_or_url or "").strip()
    if not s:
        return ""
    s = s.lower().replace("doi:", "").strip()
    s = s.replace("https://doi.org/", "").replace("http://doi.org/", "")
    return s.strip()


def unpaywall_lookup(doi_or_url: str, email: str, timeout: int = 20) -> dict | None:
    """
    Unpaywall v2: https://api.unpaywall.org/v2/{DOI}?email=...   [oai_citation:4â€¡pubfetcher.readthedocs.io](https://pubfetcher.readthedocs.io/en/stable/fetcher.html?utm_source=chatgpt.com)
    """
    doi = bare_doi(doi_or_url)
    if not doi or not email:
        return None

    url = f"https://api.unpaywall.org/v2/{doi}"
    r = requests.get(url, params={"email": email}, timeout=timeout)
    if r.status_code == 404:
        return None
    r.raise_for_status()
    return r.json()


def attach_fulltext_links(cfg, items: list[dict]) -> list[dict]:
    """
    ç»™æ¯æ¡è®°å½•è¡¥ï¼š
      - pdf_urlï¼ˆè‹¥æœ‰ï¼‰
      - oa_status / license / versionï¼ˆå¯é€‰æ˜¾ç¤ºï¼‰
    åªå¯¹â€œå·²ç»å…¥é€‰è¦å‘é‚®ä»¶çš„æ¡ç›®â€åšæŸ¥è¯¢ï¼Œæ§åˆ¶è°ƒç”¨é‡ï¼ˆå»ºè®®â‰¤10ä¸‡/å¤©/ç”¨æˆ·ï¼‰ã€‚ [oai_citation:5â€¡docs.ropensci.org](https://docs.ropensci.org/roadoi/reference/oadoi_fetch.html)
    """
    email = (os.getenv("UNPAYWALL_EMAIL") or "").strip()
    if not email:
        print("Unpaywall: UNPAYWALL_EMAIL missing; skip fulltext enrichment.")
        return items

    cache: dict[str, dict] = {}
    for it in items:
        d = bare_doi(it.get("doi") or "")
        if not d:
            continue

        if d in cache:
            data = cache[d]
        else:
            try:
                data = unpaywall_lookup(d, email, timeout=int(cfg.get("unpaywall_timeout", 20)))
            except Exception as e:
                print(f"Unpaywall error for DOI {d}: {e}")
                data = None
            cache[d] = data or {}
            time.sleep(0.12)  # è½»å¾®é™é€Ÿï¼Œç¤¼è²Œä¸€ç‚¹

        if not data:
            continue

        best = data.get("best_oa_location") or {}
        pdf = best.get("url_for_pdf") or ""   # å­—æ®µååœ¨ Unpaywall schema/æ”¯æŒæ–‡æ¡£é‡Œåˆ—å‡º  [oai_citation:6â€¡Unpaywall](https://support.unpaywall.org/support/solutions/articles/44002142311-what-do-the-fields-in-the-api-response-and-snapshot-records-mean-)
        landing = best.get("url_for_landing_page") or ""
        it["pdf_url"] = pdf or ""
        it["oa_status"] = data.get("oa_status") or ""
        it["oa_license"] = best.get("license") or ""
        it["oa_version"] = best.get("version") or ""
        it["oa_landing"] = landing or ""

    return items




# -------------------------
# ai4scholar API
# -------------------------
def ai4s_headers():
    key = (os.getenv("AI4SCHOLAR_API_KEY") or "").strip()
    if not key:
        return None
    return {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}


def fetch_ai4s_recommendations_from_seeds(cfg) -> tuple[bool, list[dict]]:
    """
    ai4scholar ä¼˜å…ˆå…¥å£ï¼š
    - æˆåŠŸï¼ˆHTTP 200ï¼‰=> è¿”å› (True, recs)ï¼Œå¹¶â€œè·³è¿‡â€å®˜æ–¹ S2
    - å¤±è´¥/å¼‚å¸¸ => è¿”å› (False, [])ï¼Œè®©å¤–å±‚ fallback åˆ°å®˜æ–¹ S2

    è¯´æ˜ï¼š
    ai4scholar æ–‡æ¡£ç¤ºä¾‹æ˜¾ç¤ºç”¨ Authorization: Bearer æ–¹å¼è®¿é—® /graph/v1/...  [oai_citation:2â€¡Awesomely](https://ai4scholar.net/docs/code-examples)
    æˆ‘ä»¬è¿™é‡ŒæŒ‰ Semantic Scholar Recommendations çš„è·¯å¾„å»å°è¯•ï¼š/recommendations/v1/papers(/)
    """
    headers = ai4s_headers()
    if not headers:
        return (False, [])

    pos = load_seed_dois("seeds_positive.txt")
    neg = load_seed_dois("seeds_negative.txt")
    positive = [doi_to_s2_pid(d) for d in pos if doi_to_s2_pid(d)]
    negative = [doi_to_s2_pid(d) for d in neg if doi_to_s2_pid(d)]
    if not positive:
        return (True, [])  # æœ‰ key ä½†æ²¡æœ‰æ­£ä¾‹ï¼šè§†ä¸ºâ€œæˆåŠŸä½†æ— è¾“å‡ºâ€ï¼Œè·³è¿‡å®˜æ–¹ S2

    base = "https://ai4scholar.net"
    url = f"{base}/recommendations/v1/papers/"  # å°¾æ–œæ æ›´ç¨³
    params = {
        "fields": "title,abstract,year,citationCount,venue,externalIds,url",
        "limit": int(cfg.get("s2_limit", 20)),
    }
    payload = {"positivePaperIds": positive, "negativePaperIds": negative}

    retries = int(cfg.get("s2_retries", 2))
    base_backoff = int(cfg.get("s2_backoff_sec", 3))

    for attempt in range(retries + 1):
        try:
            r = requests.post(url, headers=headers, params=params, data=json.dumps(payload), timeout=60)

            # æ‰“å°ç§¯åˆ†ä¿¡æ¯ï¼ˆai4scholar ç¤ºä¾‹é‡Œæåˆ°è¿™äº› headersï¼‰ [oai_citation:3â€¡Awesomely](https://ai4scholar.net/docs/code-examples)
            if r.status_code == 200:
                rem = r.headers.get("X-Credits-Remaining")
                charged = r.headers.get("X-Credits-Charged")
                print(f"AI4S: ok, remaining={rem}, charged={charged}")

                data = r.json()
                # å…¼å®¹ä¸¤ç§å¯èƒ½çš„è¿”å›ç»“æ„ï¼šrecommendedPapersï¼ˆS2é£æ ¼ï¼‰ æˆ– dataï¼ˆai4sé£æ ¼ï¼‰
                recs = data.get("recommendedPapers", None)
                if recs is None:
                    recs = data.get("data", []) or []

                # æ ‡è®°æ¥æºï¼Œä¾¿äºä½ åœ¨é‚®ä»¶é‡Œæ˜¾ç¤ºâ€œvia ai4scholarâ€
                for p in recs:
                    if isinstance(p, dict):
                        p["_via"] = "ai4scholar"

                return (True, recs)

            # 429/5xxï¼šé‡è¯•
            if r.status_code == 429 or 500 <= r.status_code < 600:
                if attempt < retries:
                    sleep_s = base_backoff * (2 ** attempt)
                    print(f"AI4S: {r.status_code}; retry in {sleep_s}s")
                    time.sleep(sleep_s)
                    continue
                print(f"AI4S: failed status={r.status_code}; fallback to official S2.")
                return (False, [])

            # 401/402/403 ç­‰ï¼šç›´æ¥ fallbackï¼ˆ401/402 åœ¨ ai4scholar æ–‡æ¡£ç¤ºä¾‹é‡Œæœ‰æåˆ°ï¼‰ [oai_citation:4â€¡Awesomely](https://ai4scholar.net/docs/code-examples)
            print(f"AI4S: failed status={r.status_code}; fallback to official S2.")
            return (False, [])

        except Exception as e:
            if attempt < retries:
                sleep_s = base_backoff * (2 ** attempt)
                print(f"AI4S: exception {e}; retry in {sleep_s}s")
                time.sleep(sleep_s)
                continue
            print(f"AI4S: exception {e}; fallback to official S2.")
            return (False, [])

    return (False, [])







# -------------------------
# enrich / å»é‡ / æ’åº
# -------------------------
def enrich(cfg, works: list[dict], tag: str = "") -> list[dict]:
    out = []
    for w in works:
        title = w.get("title") or ""
        abstract = reconstruct_abstract(w.get("abstract_inverted_index"))
        if excluded(title, abstract, cfg.get("exclude_keywords", [])):
            continue

        out.append({
            "title": title,
            "abstract": abstract,
            "publication_year": w.get("publication_year"),
            "publication_date": w.get("publication_date"),
            "cited_by_count": w.get("cited_by_count", 0) or 0,
            "venue": (((w.get("primary_location") or {}).get("source") or {}).get("display_name")) or "",
            "doi": w.get("doi"),
            "url": pick_best_url(w),
            "relevance": relevance_score(title, abstract, cfg["keywords"]),
            "bucket": tag,  # latest / classic / reco
            "via": w.get("_via", "official_s2"),
        })
    return out


def dedupe(items: list[dict]) -> list[dict]:
    seen = set()
    out = []
    for it in items:
        key = it.get("doi") or it.get("url") or it.get("title")
        if not key or key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out


def load_seen(path="seen.json") -> dict:
    if not os.path.exists(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}
    


def save_seen(seen: dict, path="seen.json"):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(seen, f, ensure_ascii=False, indent=2)


def filter_seen(cfg, items: list[dict], seen: dict) -> list[dict]:
    keep_days = int(cfg.get("seen_days_keep", 30))
    today = dt.date.today()

    # æ¸…ç†è¿‡æœŸè®°å½•
    cleaned = {}
    for k, v in seen.items():
        try:
            d = dt.date.fromisoformat(v)
            if (today - d).days <= keep_days:
                cleaned[k] = v
        except Exception:
            pass
    seen.clear()
    seen.update(cleaned)

    out = []
    for it in items:
        key = it.get("doi") or it.get("url") or it.get("title")
        if not key:
            continue
        if key in seen:
            continue
        out.append(it)
    return out




def pick_top(items: list[dict], n: int) -> list[dict]:
    # ç®€å•å¯ç”¨ï¼šç›¸å…³æ€§ä¼˜å…ˆï¼Œå†çœ‹å¼•ç”¨æ•°
    items = sorted(items, key=lambda x: (x["relevance"], x["cited_by_count"]), reverse=True)
    return items[:n]


def pick_top_cited(items: list[dict], n: int) -> list[dict]:
    return sorted(items, key=lambda x: x.get("cited_by_count", 0), reverse=True)[:n]


def enrich_s2(cfg, papers: list[dict], tag: str = "reco_s2") -> list[dict]:
    out = []
    for p in papers:
        title = p.get("title") or ""
        abstract = p.get("abstract") or ""

        if excluded(title, abstract, cfg.get("exclude_keywords", [])):
            continue

        ext = p.get("externalIds") or {}
        doi = ext.get("DOI") or ""
        doi_url = f"https://doi.org/{doi}" if doi else ""

        url = p.get("url") or doi_url

        out.append({
            "title": title,
            "abstract": abstract,
            "publication_year": p.get("year"),
            "publication_date": None,
            "cited_by_count": p.get("citationCount", 0) or 0,
            "venue": p.get("venue") or "",
            "doi": doi_url,
            "url": url or doi_url,
            "relevance": relevance_score(title, abstract, cfg["keywords"]),
            "bucket": tag,  # reco_s2
        })
    return out



# -------------------------
# é‚®ä»¶ HTML
# -------------------------
def build_html(
    cfg,
    latest: list[dict],
    classic: list[dict],
    reco_s2: list[dict],
    reco_oa: list[dict],
) -> str:
    date_str = now_local(cfg["timezone"]).strftime("%Y-%m-%d (%a)")
    build_sha = (os.getenv("GITHUB_SHA", "") or "")[:7]
    run_id = os.getenv("GITHUB_RUN_ID", "")
    
    def card(it: dict) -> str:
        brief = human_brief_cn(it["title"], it["abstract"]).replace("\n", "<br>")

        # æ¥æºæ ‡ç­¾
        source_label = "å…³é”®è¯"
        if it.get("bucket") == "reco_s2":
            via = it.get("via", "official_s2")
            source_label = "S2çŒœä½ å–œæ¬¢(ai4scholar)" if via == "ai4scholar" else "S2çŒœä½ å–œæ¬¢(å®˜æ–¹)"
        elif it.get("bucket") == "reco_oa":
            source_label = "OpenAlexç›¸å…³"
        elif it.get("bucket") == "reco":
            source_label = "æ¨è"
        elif it.get("bucket") == "latest":
            source_label = "æœ€æ–°"
        elif it.get("bucket") == "classic":
            source_label = "ç»å…¸"

        # æ ‡é¢˜æ°¸è¿œæŒ‡å‘ DOI/è½åœ°é¡µï¼›PDF ä½œä¸ºå¯é€‰æŒ‰é’®
        doi_url = it.get("url") or ""
        pdf_url = it.get("pdf_url") or ""

        pdf_btn = ""
        if pdf_url:
            pdf_btn = f"""
              <a href="{pdf_url}" target="_blank" rel="noreferrer"
                 style="display:inline-block;margin-left:8px;padding:2px 10px;border:1px solid #888;border-radius:999px;text-decoration:none;font-weight:600;">
                PDF
              </a>
            """

        return f"""
        <div style="margin:14px 0;padding:12px;border:1px solid #ddd;border-radius:10px;">
          <div style="font-size:16px;font-weight:700;">
            <a href="{doi_url}" target="_blank" rel="noreferrer">{it['title']}</a>
            {pdf_btn}
          </div>
          <div style="color:#555;margin-top:6px;">
            {it['venue'] or 'Unknown venue'} Â· {it['publication_year'] or ''} Â· å¼•ç”¨ {it['cited_by_count']} Â· relevance {it['relevance']} Â· æ¥æº {source_label} Â· å…¨æ–‡ {"PDF" if pdf_url else "æ— "}
          </div>
          <div style="margin-top:10px;line-height:1.55;">{brief}</div>
        </div>
        """


    reco_days = ""
    return f"""
    <html><body style="font-family:Arial, Helvetica, sans-serif;">
      <h2>{cfg['topic_cn']} â€” æ¯æ—¥ç§‘ç ”ç®€æŠ¥ï¼ˆ{date_str}ï¼‰</h2>
      <p style="color:#666;">
        æ•°æ®æºï¼šOpenAlexï¼ˆworks æœç´¢ + å¼•ç”¨æ•° + related_works æ¨èï¼‰ã€‚å»ºè®®å¸¦ OPENALEX_MAILTO åš polite usageã€‚<br>
        æ„å»ºæ ‡è¯†ï¼šsha={build_sha} run={run_id}
      </p>

      <h3>â­ S2çŒœä½ å–œæ¬¢ï¼ˆæ›´åƒâ€œä½ å¯èƒ½ä¹Ÿå–œæ¬¢â€ï¼‰</h3>
      {''.join(card(x) for x in reco_s2) if reco_s2 else '<p>S2 ä»Šå¤©æ²¡æœ‰äº§å‡ºï¼ˆæˆ–è¢«è·³è¿‡ï¼‰ï¼Œä¸å½±å“å…¶ä»–å†…å®¹ã€‚</p>'}

      <h3>ğŸ§­ OpenAlexè„‰ç»œï¼ˆæ²¿ä½ çš„ç§å­è®ºæ–‡ç›¸å…³å›¾è°±æ‰©å±•ï¼‰</h3>
      {''.join(card(x) for x in reco_oa) if reco_oa else '<p>OpenAlex related_works ä»Šå¤©ä¸ºç©ºï¼šæ£€æŸ¥ seeds_positive.txt DOI æ˜¯å¦æœ‰æ•ˆã€‚</p>'}

      <h3>ğŸ†• æœ€æ–°è¿›å±•ï¼ˆè¿‘ {cfg['latest_days']} å¤©ï¼‰</h3>
      {''.join(card(x) for x in latest) if latest else '<p>ä»Šå¤©æœªæŠ“åˆ°è¶³å¤ŸåŒ¹é…çš„æœ€æ–°æ¡ç›®ã€‚</p>'}

      <h3>ğŸ›ï¸ ç»å…¸/é«˜å½±å“åŠ›ï¼ˆä¸¤å¹´å‰åŠæ›´æ—©ï¼‰</h3>
      {''.join(card(x) for x in classic) if classic else '<p>ä»Šå¤©æœªæŠ“åˆ°è¶³å¤ŸåŒ¹é…çš„ç»å…¸æ¡ç›®ã€‚</p>'}

      <hr>
      <p style="color:#888;font-size:12px;">
        ä¸‹ä¸€é˜¶æ®µï¼šæ¥å…¥ Semantic Scholar Recommendationsï¼ˆæ”¯æŒæ­£/è´Ÿä¾‹æ›´æ‡‚ä½ ï¼‰ï¼Œå¹¶æŠŠæ‘˜è¦å‡çº§ä¸ºâ€œå¯é€‰å¤§æ¨¡å‹ç”Ÿæˆï¼ˆåªå¯¹ Top-N è°ƒç”¨ï¼Œæ§åˆ¶ token æˆæœ¬ï¼‰â€ã€‚
      </p>
    </body></html>
    """


def send_email(subject: str, html: str):
    host = os.environ["SMTP_HOST"]
    port = int(os.getenv("SMTP_PORT", "587"))
    user = os.environ["SMTP_USER"]
    pw = os.environ["SMTP_PASS"]
    to_email = os.environ["TO_EMAIL"]

    msg = MIMEMultipart("alternative")
    msg["Subject"] = subject
    msg["From"] = user
    msg["To"] = to_email
    msg.attach(MIMEText(html, "html", "utf-8"))

    with smtplib.SMTP(host, port) as s:
        s.ehlo()
        s.starttls()
        s.login(user, pw)
        s.sendmail(user, [to_email], msg.as_string())


def main():
    cfg = load_config()
    if not should_send_now(cfg):
        print("Not sending now (local hour mismatch).")
        return

    mailto = os.getenv("OPENALEX_MAILTO", "")
    seen = load_seen()
    print(f"DEBUG seen loaded: {len(seen)}")

    # 1) å…³é”®è¯ï¼šæœ€æ–° + ç»å…¸
    # 1) å…³é”®è¯ï¼šæœ€æ–° + ç»å…¸
    latest_raw, classic_raw = fetch_latest_and_classic(cfg, mailto)
    latest_items = filter_seen(cfg, dedupe(enrich(cfg, latest_raw, "latest")), seen)
    classic_items = filter_seen(cfg, dedupe(enrich(cfg, classic_raw, "classic")), seen)
    latest = pick_top(latest_items, int(cfg["top_latest"]))
    classic = pick_top(classic_items, int(cfg["top_classic"]))

    # 2) Milestone Bï¼šDOI seeds -> related_works æ¨è
    # OpenAlex æ¨èï¼ˆä½ å·²å®Œæˆï¼‰
    reco_oa_raw = fetch_recommendations_from_seeds(cfg, mailto)
    reco_oa = dedupe(enrich(cfg, reco_oa_raw, "reco_oa"))
    reco_oa = filter_seen(cfg,reco_oa,seen)
    reco_oa = pick_top_cited(reco_oa, int(cfg.get("top_reco_oa", 10)))
    
    # S2 æ¨èï¼ˆæ—  key ä¹Ÿå°è¯•ï¼›å¤±è´¥ä¼šè‡ªåŠ¨è·³è¿‡ï¼‰
    reco_s2_raw = fetch_s2_recommendations_from_seeds(cfg)
    reco_s2 = dedupe(enrich_s2(cfg, reco_s2_raw, "reco_s2"))
    reco_s2 = filter_seen(cfg,reco_s2,seen)
    reco_s2 = pick_top_cited(reco_s2, int(cfg.get("top_reco_s2", 10)))
    
    # åˆå¹¶å»é‡
    reco_all = dedupe(reco_s2 + reco_oa)
    
    # è½»å¾®åå‘ S2ï¼ˆå› ä¸ºæ›´åƒâ€œçŒœä½ å–œæ¬¢â€ï¼‰ï¼›æ—  S2 æ•°æ®ä¹Ÿä¸å½±å“
    for it in reco_all:
        if it.get("bucket") == "reco_s2":
            it["relevance"] += 2
    
    reco = pick_top(reco_all, int(cfg.get("top_reco", 3)))

    latest = attach_fulltext_links(cfg, latest)
    classic = attach_fulltext_links(cfg, classic)
    reco_s2 = attach_fulltext_links(cfg, reco_s2)
    reco_oa = attach_fulltext_links(cfg, reco_oa)

    html = build_html(cfg, latest, classic, reco_s2, reco_oa)
    subject = f"[æ¯æ—¥ç§‘ç ”ç®€æŠ¥] {cfg['topic_cn']} | {now_local(cfg['timezone']).strftime('%Y-%m-%d')}"

    send_email(subject, html)
    today_str = dt.date.today().isoformat()
    for lst in [latest, classic, reco_s2, reco_oa]:
        for it in lst:
            k = it.get("doi") or it.get("url") or it.get("title")
            if k:
                seen[k] = today_str
    save_seen(seen)
    print(f"DEBUG seen saved: {len(seen)}")
    print("Email sent.")


if __name__ == "__main__":
    main()
