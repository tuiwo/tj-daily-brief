import os
import re
import time
import math
import smtplib
import datetime as dt
from zoneinfo import ZoneInfo
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import json

import requests
import yaml


# -------------------------
# åŸºç¡€ï¼šè¯»å– config
# -------------------------
def load_config(path="config.yml"):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def now_local(tz: str) -> dt.datetime:
    return dt.datetime.now(ZoneInfo(tz))


def should_send_now(cfg) -> bool:
    # åªåœ¨æœ¬åœ°æŒ‡å®šå°æ—¶å‘ä¿¡ï¼ˆç”¨äºé…åˆ UTC 15/16 åŒè·‘ï¼‰
    return now_local(cfg["timezone"]).hour == int(cfg["send_hour_local"])


# -------------------------
# OpenAlexï¼šæŠ½è±¡è¿˜åŸï¼ˆabstract_inverted_index -> stringï¼‰
# -------------------------
def reconstruct_abstract(inv_idx):
    if not inv_idx:
        return ""
    pos2word = {}
    for word, poses in inv_idx.items():
        for p in poses:
            pos2word[p] = word
    return " ".join(pos2word[i] for i in sorted(pos2word))


def openalex_get(params):
    r = requests.get("https://api.openalex.org/works", params=params, timeout=60)
    r.raise_for_status()
    return r.json()


def openalex_get_work_by_id(openalex_id: str, mailto: str = "") -> dict | None:
    """
    openalex_id é€šå¸¸é•¿è¿™æ ·ï¼š
      https://openalex.org/Wxxxxxxxxx
    æˆ‘ä»¬æŠŠå®ƒè½¬æ¢ä¸º APIï¼š
      https://api.openalex.org/works/Wxxxxxxxxx
    """
    if not openalex_id:
        return None
    oid = openalex_id.strip()
    if oid.startswith("https://openalex.org/"):
        work_id = oid.split("/")[-1]  # Wxxxx
    else:
        work_id = oid  # ä¹Ÿå¯èƒ½ç›´æ¥ç»™ Wxxxx

    url = f"https://api.openalex.org/works/{work_id}"
    params = {}
    if mailto:
        params["mailto"] = mailto
    r = requests.get(url, params=params, timeout=60)
    if r.status_code == 404:
        return None
    r.raise_for_status()
    return r.json()


def normalize_doi(doi: str) -> str:
    """
    OpenAlex çš„ doi å­—æ®µä¸€èˆ¬æ˜¯å®Œæ•´ URL å½¢å¼ï¼šhttps://doi.org/...
    è¿™é‡ŒæŠŠç”¨æˆ·è¾“å…¥çš„ DOI è§„èŒƒæˆè¿™ç§å½¢å¼ï¼Œä¾¿äº filter=doi:...
    """
    d = (doi or "").strip()
    if not d:
        return ""
    d = d.lower()
    d = d.replace("doi:", "").strip()
    if d.startswith("http://"):
        d = "https://" + d[len("http://"):]
    if d.startswith("https://doi.org/"):
        return d
    return "https://doi.org/" + d


def openalex_find_work_by_doi(doi: str, mailto: str = "") -> dict | None:
    """
    ç”¨ filter=doi:... æ‰¾åˆ°å¯¹åº” work
    """
    doi_url = normalize_doi(doi)
    if not doi_url:
        return None
    params = {"filter": f"doi:{doi_url}", "per_page": 1}
    if mailto:
        params["mailto"] = mailto
    data = openalex_get(params)
    results = data.get("results", [])
    return results[0] if results else None


def pick_best_url(work: dict) -> str:
    doi = work.get("doi")
    if doi:
        return doi
    primary = (work.get("primary_location") or {}).get("landing_page_url")
    if primary:
        return primary
    return work.get("id", "")


def normalize(s: str) -> str:
    return (s or "").lower()


# -------------------------
# ç›¸å…³æ€§ä¸è§„åˆ™æ‘˜è¦
# -------------------------
def relevance_score(title: str, abstract: str, keywords: list[str]) -> int:
    t = normalize(title)
    a = normalize(abstract)
    score = 0
    for kw in keywords:
        k = kw.lower()
        if k in t:
            score += 3
        elif k in a:
            score += 1
    return score


def excluded(title: str, abstract: str, exclude_keywords: list[str]) -> bool:
    t = normalize(title)
    a = normalize(abstract)
    return any(k.lower() in t or k.lower() in a for k in exclude_keywords)


def extract_numbers(text: str) -> str:
    hits = re.findall(r"(\d+(?:\.\d+)?\s*(?:Â°c|â„ƒ|k|%))", normalize(text))
    uniq = []
    for h in hits:
        h = h.replace(" ", "")
        if h not in uniq:
            uniq.append(h)
    return ", ".join(uniq[:6])


def guess_tags(text: str) -> list[str]:
    t = normalize(text)
    tags = []
    mapping = [
        ("TSEP", ["tsep", "temperature sensitive electrical parameter"]),
        ("ç”µå‚æ³•(Vce/Vf/Rds)", ["vce", "vce(sat)", "vf", "forward voltage", "rds(on)"]),
        ("ç”µçƒ­æ¨¡å‹/çƒ­é˜»æŠ—", ["electro-thermal", "thermal impedance", "foster", "cauer"]),
        ("æ»¤æ³¢/ä¼°è®¡", ["kalman", "ukf", "ekf", "observer", "state estimation"]),
        ("å™¨ä»¶:SiC", ["sic"]),
        ("å™¨ä»¶:IGBT", ["igbt"]),
        ("æ¨¡å—/å°è£…", ["power module", "module", "packaging"]),
    ]
    for name, keys in mapping:
        if any(k in t for k in keys):
            tags.append(name)
    return tags[:4]


def human_brief_cn(title: str, abstract: str) -> str:
    tags = guess_tags(title + " " + abstract)
    nums = extract_numbers(abstract)

    sents = re.split(r"(?<=[.!?])\s+", abstract.strip())
    sents = [s for s in sents if len(s) > 40]
    explain = " ".join(sents[:2]) if sents else "ï¼ˆæ‘˜è¦ä¿¡æ¯ä¸è¶³ï¼šå»ºè®®ç‚¹å¼€é“¾æ¥å¿«é€Ÿåˆ¤æ–­æ˜¯å¦ä¸ä½ çš„åœ¨çº¿ç›‘æµ‹é“¾è·¯ç›¸å…³ã€‚ï¼‰"

    return "\n".join([
        "ä¸€å¥è¯ï¼šè¿™ç¯‡å·¥ä½œå›´ç»•ç»“æ¸©åœ¨çº¿ä¼°ç®—/ç›‘æµ‹ç»™å‡ºä¸€æ¡å¯å®ç°çš„æŠ€æœ¯è·¯å¾„ã€‚",
        f"æ–¹æ³•çº¿ç´¢ï¼š{(' / '.join(tags)) if tags else 'æœªä»æ‘˜è¦é‡Œè¯†åˆ«åˆ°æ˜ç¡®æ–¹æ³•å…³é”®è¯'}",
        f"å¯é‡åŒ–æŒ‡æ ‡ï¼š{nums if nums else 'æ‘˜è¦æœªç»™å‡ºæ˜ç¡®æ•°å€¼ï¼ˆæˆ–éœ€è¯»å…¨æ–‡/å›¾è¡¨ï¼‰'}",
        f"æ‹†è§£ï¼š{explain}",
        "å»ºè®®ï¼šå¦‚æœä½ åœ¨åš TSEP æ ‡å®š/åœ¨çº¿ä¼°ç®—é“¾è·¯/è¯¯å·®è¯„ä¼°ï¼Œè¿™ç¯‡ä¼˜å…ˆè¯»ï¼›å¦åˆ™å…ˆæ”¶è—è§‚å¯Ÿã€‚"
    ])


# -------------------------
# å€™é€‰è·å–ï¼šå…³é”®è¯ï¼ˆæœ€æ–°/ç»å…¸ï¼‰
# -------------------------
def fetch_latest_and_classic(cfg, mailto: str):
    query = cfg.get("search_query") or " ".join(cfg["keywords"][:6])

    today = dt.date.today()
    from_date = (today - dt.timedelta(days=int(cfg["latest_days"]))).isoformat()
    classic_to = (today - dt.timedelta(days=365 * 2)).isoformat()

    common_filter = "type:journal-article|proceedings-article"

    base = {"search": query, "per_page": 50}
    if mailto:
        base["mailto"] = mailto

    latest = openalex_get({
        **base,
        "filter": f"from_publication_date:{from_date},{common_filter}",
        "sort": "publication_date:desc",
    }).get("results", [])

    classic = openalex_get({
        **base,
        "filter": f"to_publication_date:{classic_to},{common_filter}",
        "sort": "cited_by_count:desc",
    }).get("results", [])

    return latest, classic


# -------------------------
# Milestone Bï¼šDOI seeds -> related_works æ¨è
# -------------------------
def load_seed_dois(path: str) -> list[str]:
    if not os.path.exists(path):
        return []
    out = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            s = line.strip()
            if not s or s.startswith("#"):
                continue
            out.append(s)
    return out


def fetch_recommendations_from_seeds(cfg, mailto: str) -> list[dict]:
    """
    å¯¹æ¯ä¸ª seed DOIï¼š
      DOI -> OpenAlex work
      work.related_works -> æ‹‰å›ç›¸å…³ works
    æœ€ååˆå¹¶å»é‡ï¼Œå¹¶æ‰“ä¸Š reco_source æ ‡è®°
    """
    pos = load_seed_dois("seeds_positive.txt")
    neg = set(normalize_doi(x) for x in load_seed_dois("seeds_negative.txt"))

    if not pos:
        return []

    max_related = int(cfg.get("max_related_per_seed", 25))
    all_ids: list[str] = []
    seed_doi_urls = set()

    # 1) æ¯ä¸ª DOI æ‰¾åˆ°å¯¹åº” workï¼Œå¹¶æ”¶é›† related_works ids
    for doi in pos:
        w = openalex_find_work_by_doi(doi, mailto)
        time.sleep(0.12)  # è½»å¾®é™é€Ÿï¼Œå‡å°‘è¢«é™æµé£é™©
        if not w:
            continue
        doi_url = w.get("doi")
        if doi_url:
            seed_doi_urls.add(doi_url)

        rel = w.get("related_works") or []
        all_ids.extend(rel[:max_related])

    # 2) æ‹‰å› related works è¯¦æƒ…ï¼ˆé€ä¸ªæ‹‰ï¼Œé‡ä¸å¤§æ›´ç¨³ï¼‰
    recos = []
    seen = set()
    for oid in all_ids:
        if oid in seen:
            continue
        seen.add(oid)
        w = openalex_get_work_by_id(oid, mailto)
        time.sleep(0.12)
        if not w:
            continue
        # æ’é™¤ï¼šè´Ÿä¾‹ DOIã€ä»¥åŠç§å­æœ¬èº«
        doi_url = w.get("doi") or ""
        if doi_url and (doi_url in neg or doi_url in seed_doi_urls):
            continue
        recos.append(w)

    return recos



# -------------------------
# milestone C: wo API
# -------------------------

def s2_headers():
    # æ²¡ key ä¹Ÿèƒ½å°è¯•ï¼›æœ‰ key ä¼šæ›´ç¨³ï¼ˆå®˜æ–¹å»ºè®®ä½¿ç”¨ keyï¼‰ [oai_citation:4â€¡Semantic Scholar](https://www.semanticscholar.org/product/api%2Ftutorial?utm_source=chatgpt.com)
    key = (os.getenv("S2_API_KEY") or "").strip()
    h = {"Content-Type": "application/json"}
    if key:
        h["x-api-key"] = key
    return h


def doi_to_s2_pid(doi: str) -> str:
    """
    æŠŠ DOI è§„èŒƒæˆ Semantic Scholar æ¨èæ¥å£å¸¸ç”¨çš„ paperId å½¢å¼ï¼šDOI:10.xxxx/xxxx
    """
    d = (doi or "").strip()
    d = d.replace("doi:", "").strip()
    d = d.replace("https://doi.org/", "").strip()
    d = d.replace("http://doi.org/", "").strip()
    return f"DOI:{d}" if d else ""


def fetch_s2_recommendations_from_seeds(cfg) -> list[dict]:
    """
    Semantic Scholar Recommendations APIï¼š
      POST https://api.semanticscholar.org/recommendations/v1/papers
    å®˜æ–¹æœ‰ Recommendations API æ–‡æ¡£ã€‚ [oai_citation:5â€¡è¯­ä¹‰å­¦è€…](https://api.semanticscholar.org/api-docs/recommendations?utm_source=chatgpt.com)

    æ—  keyï¼šæ›´å¯èƒ½ 429/å¤±è´¥ï¼Œæ‰€ä»¥è¿™é‡Œåšï¼š
      - å° limit
      - é‡è¯• + æŒ‡æ•°é€€é¿
      - å¤±è´¥ç›´æ¥è¿”å›ç©ºåˆ—è¡¨ï¼ˆä¸å½±å“é‚®ä»¶ï¼‰
    """
    if not cfg.get("use_s2_recommendations", True):
        return []

    pos = load_seed_dois("seeds_positive.txt")
    neg = load_seed_dois("seeds_negative.txt")

    positive = [doi_to_s2_pid(d) for d in pos if doi_to_s2_pid(d)]
    negative = [doi_to_s2_pid(d) for d in neg if doi_to_s2_pid(d)]

    if not positive:
        return []

    url = "https://api.semanticscholar.org/recommendations/v1/papers"
    params = {
        "fields": "title,abstract,year,citationCount,venue,externalIds,url",
        "limit": int(cfg.get("s2_limit", 20)),
    }

    payload = {"positivePaperIds": positive, "negativePaperIds": negative}

    retries = int(cfg.get("s2_retries", 2))
    base_backoff = int(cfg.get("s2_backoff_sec", 3))

    for attempt in range(retries + 1):
        try:
            r = requests.post(
                url,
                headers=s2_headers(),
                params=params,
                data=json.dumps(payload),
                timeout=60,
            )

            # 429/5xxï¼šé‡è¯•ï¼ˆæ—  key æ—¶æ›´å¸¸è§ï¼‰ [oai_citation:6â€¡Semantic Scholar](https://www.semanticscholar.org/product/api%2Ftutorial?utm_source=chatgpt.com)
            if r.status_code == 429 or 500 <= r.status_code < 600:
                if attempt < retries:
                    sleep_s = base_backoff * (2 ** attempt)
                    print(f"S2 rate-limited or server error ({r.status_code}); retry in {sleep_s}s")
                    time.sleep(sleep_s)
                    continue
                print(f"S2 failed with status={r.status_code}; skipping.")
                return []

            r.raise_for_status()
            data = r.json()
            return data.get("recommendedPapers", []) or []

        except Exception as e:
            if attempt < retries:
                sleep_s = base_backoff * (2 ** attempt)
                print(f"S2 exception: {e}; retry in {sleep_s}s")
                time.sleep(sleep_s)
                continue
            print(f"S2 exception: {e}; skipping.")
            return []

    return []






# -------------------------
# enrich / å»é‡ / æ’åº
# -------------------------
def enrich(cfg, works: list[dict], tag: str = "") -> list[dict]:
    out = []
    for w in works:
        title = w.get("title") or ""
        abstract = reconstruct_abstract(w.get("abstract_inverted_index"))
        if excluded(title, abstract, cfg.get("exclude_keywords", [])):
            continue

        out.append({
            "title": title,
            "abstract": abstract,
            "publication_year": w.get("publication_year"),
            "publication_date": w.get("publication_date"),
            "cited_by_count": w.get("cited_by_count", 0) or 0,
            "venue": (((w.get("primary_location") or {}).get("source") or {}).get("display_name")) or "",
            "doi": w.get("doi"),
            "url": pick_best_url(w),
            "relevance": relevance_score(title, abstract, cfg["keywords"]),
            "bucket": tag,  # latest / classic / reco
        })
    return out


def dedupe(items: list[dict]) -> list[dict]:
    seen = set()
    out = []
    for it in items:
        key = it.get("doi") or it.get("url") or it.get("title")
        if not key or key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out


def pick_top(items: list[dict], n: int) -> list[dict]:
    # ç®€å•å¯ç”¨ï¼šç›¸å…³æ€§ä¼˜å…ˆï¼Œå†çœ‹å¼•ç”¨æ•°
    items = sorted(items, key=lambda x: (x["relevance"], x["cited_by_count"]), reverse=True)
    return items[:n]


def enrich_s2(cfg, papers: list[dict], tag: str = "reco_s2") -> list[dict]:
    out = []
    for p in papers:
        title = p.get("title") or ""
        abstract = p.get("abstract") or ""

        if excluded(title, abstract, cfg.get("exclude_keywords", [])):
            continue

        ext = p.get("externalIds") or {}
        doi = ext.get("DOI") or ""
        doi_url = f"https://doi.org/{doi}" if doi else ""

        url = p.get("url") or doi_url

        out.append({
            "title": title,
            "abstract": abstract,
            "publication_year": p.get("year"),
            "publication_date": None,
            "cited_by_count": p.get("citationCount", 0) or 0,
            "venue": p.get("venue") or "",
            "doi": doi_url,
            "url": url or doi_url,
            "relevance": relevance_score(title, abstract, cfg["keywords"]),
            "bucket": tag,  # reco_s2
        })
    return out



# -------------------------
# é‚®ä»¶ HTML
# -------------------------
def build_html(cfg, latest: list[dict], classic: list[dict], reco: list[dict]) -> str:
    date_str = now_local(cfg["timezone"]).strftime("%Y-%m-%d (%a)")

    def card(it: dict) -> str:
        brief = human_brief_cn(it["title"], it["abstract"]).replace("\n", "<br>")
        source_label = "å…³é”®è¯"
        if it.get("bucket") == "reco_s2":
            source_label = "S2çŒœä½ å–œæ¬¢"
        elif it.get("bucket") == "reco_oa":
            source_label = "OpenAlexç›¸å…³"
        elif it.get("bucket") == "reco":
            source_label = "æ¨è"
        elif it.get("bucket") == "latest":
            source_label = "æœ€æ–°"
        elif it.get("bucket") == "classic":
            source_label = "ç»å…¸"
        return f"""
        <div style="margin:14px 0;padding:12px;border:1px solid #ddd;border-radius:10px;">
          <div style="font-size:16px;font-weight:700;">
            <a href="{it['url']}" target="_blank" rel="noreferrer">{it['title']}</a>
          </div>
          <div style="color:#555;margin-top:6px;">
            {it['venue'] or 'Unknown venue'} Â· {it['publication_year'] or ''} Â· å¼•ç”¨ {it['cited_by_count']} Â· relevance {it['relevance']} Â· æ¥æº {source_label}
          </div>
          <div style="margin-top:10px;line-height:1.55;">{brief}</div>
        </div>
        """

    reco_days = ""
    return f"""
    <html><body style="font-family:Arial, Helvetica, sans-serif;">
      <h2>{cfg['topic_cn']} â€” æ¯æ—¥ç§‘ç ”ç®€æŠ¥ï¼ˆ{date_str}ï¼‰</h2>
      <p style="color:#666;">
        æ•°æ®æºï¼šOpenAlexï¼ˆworks æœç´¢ + å¼•ç”¨æ•° + related_works æ¨èï¼‰ã€‚å»ºè®®å¸¦ OPENALEX_MAILTO åš polite usageã€‚
      </p>

      <h3>â­ ä¸ºä½ æ¨èï¼ˆåŸºäºä½ æ”¶è—çš„ DOI ç§å­è®ºæ–‡ï¼‰</h3>
      {''.join(card(x) for x in reco) if reco else '<p>ä»Šå¤©â€œä¸ºä½ æ¨èâ€ä¸ºç©ºï¼šè¯·åœ¨ seeds_positive.txt æ·»åŠ  3â€“10 ç¯‡ä½ è®¤å¯çš„ DOIã€‚</p>'}

      <h3>ğŸ†• æœ€æ–°è¿›å±•ï¼ˆè¿‘ {cfg['latest_days']} å¤©ï¼‰</h3>
      {''.join(card(x) for x in latest) if latest else '<p>ä»Šå¤©æœªæŠ“åˆ°è¶³å¤ŸåŒ¹é…çš„æœ€æ–°æ¡ç›®ã€‚</p>'}

      <h3>ğŸ›ï¸ ç»å…¸/é«˜å½±å“åŠ›ï¼ˆä¸¤å¹´å‰åŠæ›´æ—©ï¼‰</h3>
      {''.join(card(x) for x in classic) if classic else '<p>ä»Šå¤©æœªæŠ“åˆ°è¶³å¤ŸåŒ¹é…çš„ç»å…¸æ¡ç›®ã€‚</p>'}

      <hr>
      <p style="color:#888;font-size:12px;">
        ä¸‹ä¸€é˜¶æ®µï¼šæ¥å…¥ Semantic Scholar Recommendationsï¼ˆæ”¯æŒæ­£/è´Ÿä¾‹æ›´æ‡‚ä½ ï¼‰ï¼Œå¹¶æŠŠæ‘˜è¦å‡çº§ä¸ºâ€œå¯é€‰å¤§æ¨¡å‹ç”Ÿæˆï¼ˆåªå¯¹ Top-N è°ƒç”¨ï¼Œæ§åˆ¶ token æˆæœ¬ï¼‰â€ã€‚
      </p>
    </body></html>
    """


def send_email(subject: str, html: str):
    host = os.environ["SMTP_HOST"]
    port = int(os.getenv("SMTP_PORT", "587"))
    user = os.environ["SMTP_USER"]
    pw = os.environ["SMTP_PASS"]
    to_email = os.environ["TO_EMAIL"]

    msg = MIMEMultipart("alternative")
    msg["Subject"] = subject
    msg["From"] = user
    msg["To"] = to_email
    msg.attach(MIMEText(html, "html", "utf-8"))

    with smtplib.SMTP(host, port) as s:
        s.ehlo()
        s.starttls()
        s.login(user, pw)
        s.sendmail(user, [to_email], msg.as_string())


def main():
    cfg = load_config()
    if not should_send_now(cfg):
        print("Not sending now (local hour mismatch).")
        return

    mailto = os.getenv("OPENALEX_MAILTO", "")

    # 1) å…³é”®è¯ï¼šæœ€æ–° + ç»å…¸
    latest_raw, classic_raw = fetch_latest_and_classic(cfg, mailto)
    latest = pick_top(dedupe(enrich(cfg, latest_raw, "latest")), int(cfg["top_latest"]))
    classic = pick_top(dedupe(enrich(cfg, classic_raw, "classic")), int(cfg["top_classic"]))

    # 2) Milestone Bï¼šDOI seeds -> related_works æ¨è
    # OpenAlex æ¨èï¼ˆä½ å·²å®Œæˆï¼‰
    reco_oa_raw = fetch_recommendations_from_seeds(cfg, mailto)
    reco_oa = enrich(cfg, reco_oa_raw, "reco_oa")
    
    # S2 æ¨èï¼ˆæ—  key ä¹Ÿå°è¯•ï¼›å¤±è´¥ä¼šè‡ªåŠ¨è·³è¿‡ï¼‰
    reco_s2_raw = fetch_s2_recommendations_from_seeds(cfg)
    reco_s2 = enrich_s2(cfg, reco_s2_raw, "reco_s2")
    
    # åˆå¹¶å»é‡
    reco_all = dedupe(reco_oa + reco_s2)
    
    # è½»å¾®åå‘ S2ï¼ˆå› ä¸ºæ›´åƒâ€œçŒœä½ å–œæ¬¢â€ï¼‰ï¼›æ—  S2 æ•°æ®ä¹Ÿä¸å½±å“
    for it in reco_all:
        if it.get("bucket") == "reco_s2":
            it["relevance"] += 2
    
    reco = pick_top(reco_all, int(cfg.get("top_reco", 3)))

    html = build_html(cfg, latest, classic, reco)
    subject = f"[æ¯æ—¥ç§‘ç ”ç®€æŠ¥] {cfg['topic_cn']} | {now_local(cfg['timezone']).strftime('%Y-%m-%d')}"

    send_email(subject, html)
    print("Email sent.")


if __name__ == "__main__":
    main()
